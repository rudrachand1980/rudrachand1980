summary(reg1)
confint(reg1,level = 0.95)
predict(reg1,interval = "predict")
library(readr)
calories_consumed <- read_csv("D:/R Datasets/R codes and Datasets/assignments/Simple Linear Regression/calories_consumed.csv")
View(calories_consumed)
View(calories_consumed)
attach(calories_consumed)
plot(`Calories Consumed`,`Weight gained (grams)`)
cor(`Calories Consumed`,`Weight gained (grams)`)
reg2<-lm(`Weight gained (grams)`~`Calories Consumed`)
summary(reg2)
predict(reg2,interval = "predict")
plot(reg2)
plot(reg2)
?predict()
predict(reg2,interval = "predict")
predict.lm(reg2)
predict.lm(reg2,interval = "predict")
deliverytime<-read.csv("D:/R Datasets/R codes and Datasets/assignments/Simple Linear Regression/delivery_time.csv")
View(deliverytime)
attach(deliverytime)
plot(Delivery.Time,Sorting.Time)
cor(Delivery.Time,Sorting.Time)
rudra<-lm(Delivery.Time~Sorting.Time)
summary(rudra)
rudra<-lm(log(Delivery.Time)~log(Sorting.Time))
summary(rudra)
rudra<-lm(log(log(Delivery.Time))~log(log(Sorting.Time)))
summary(rudra)
plot(rudra)
plot(rudra)
summary(rudra)
0.84469+0.33566*10
21.00-4.20129
plot(Delivery.Time,Sorting.Time main="Scatterplot Example",
xlab="Delivery time ", ylab="sorting time ", pch=19)
plot(Delivery.Time,Sorting.Time main="Scatterplot Example",
, pch=19)
?plot
plot(rudra)
plot(Sorting.Time,Delivery.Time,xlab = "sorting-time",ylab = "delivery-time")
?plot
plot(Sorting.Time,Delivery.Time,xlab = "sorting-time",ylab = "delivery-time",asp = NA)
plot(rudra,asp = NA)
?cor
# R-squared value has increased from 0.67 to 0.7071
# Higher the R-sqaured value - Better chances of getting good model
# for Waist and addipose Tissue
plot(reg_exp)
predict(rudra,interval = "predict")
?predict
library(readr)
claimants <- read_csv("D:/R Datasets/claimants.csv")
View(claimants)
View(claimants)
attach(claimants)
View(claimants)
source('~/.active-rstudio-document', echo=TRUE)
attach(claimants)
View(claimants)
logistic_m<-glm(ATTORNEY~CLMSEX+CLMAGE+CLMINSUR+SEATBELT+LOSS)
summary(logistic_m)
exp(coef(logistic_m))
prob=predict(logistic_m,type = c("response"),claimants)
prob
prob
table(is.na(claimants$CLMAGE))
install.packages("randomForest")
library(randomForest)
B<-na.roughfix(claimants)
b
B
table(is.na(B$CLMAGE))
logistic_m<-glm(ATTORNEY~CLMSEX+CLMAGE+CLMINSUR+SEATBELT+LOSS)
summary(logistic_m)
exp(coef(logistic_m))
attach(claimants)
table(is.na(CLMAGE),SEATBELT)
table(is.na(CLMAGE),(SEATBELT))
table(is.na(CLMAGE))
library(randomForest)
B<-na.roughfix(claimants)
table(is.na(B$CLMAGE))
table(is.na(CLMAGE))
a<-na.roughfix(claimants)
View(a)
logistic_m<-glm(a$ATTORNEY~a$CLMSEX+a$CLMAGE+a$CLMINSUR+a$SEATBELT+a$LOSS)
summary(logistic_m)
exp(coef(logistic_m))
prob=predict(logistic_m,type = c("response"),claimants)
prob
output<-cbind(claimants,prob)
View(output)
confusion<-table(output$ATTORNEY,output$prob)
confusion
confusion<-table(output$ATTORNEY,output$prob,prob>0.5)
confusion
confusion<-table(output$ATTORNEY,output$prob>0.5)
confusion
confusion<-table(output$ATTORNEY,output$prob>0.50)
confusion
accuracy<-sum(diag(confusion))/sum(confusion)
accuracy
output<-cbind(claimants,output$prob>0.5)
View(output)
confusion<-table(output$ATTORNEY,output$prob,prob>0.5)
accuracy<-sum(diag(confusion))/sum(confusion)
accuracy
plot(accuracy)
library(readr)
claimants <- read_csv("D:/R Datasets/claimants.csv")
View(claimants)
View(claimants)
claimants<-claimants[-1]
View(claimants)
View(claimants)
attach(claimants)
View(claimants)
reg3<-glm(ATTORNEY~CLMAGE+CLMINSUR+CLMSEX+SEATBELT+LOSS)
summary(reg3)
exp(coef(reg3))
table(is.na(claimants$CLMAGE))
libabry(randomForest)
library(randomForest)
a<-na.roughfix(claimants)
View(a)
View(claimants)
reg4<-glm(a$ATTORNEY~a$CLMSEX+a$CLMINSUR+a$SEATBELT+a$CLMAGE+a$LOSS)
summary(reg4)
exp(coef(reg4))
prob<-predict(reg4,type = "response",a)
prob
b<-cbind(a,a$prob)
library(readr)
claimants <- read_csv("D:/R Datasets/claimants.csv")
View(claimants)
View(claimants)
claimants<-claimants[-1]
View(claimants)
attach(claimants)
fit<-glm(ATTORNEY~CLMAGE+CLMINSUR+CLMSEX+SEATBELT+LOSS)
summary(fit)
exp(coef(fit))
#conclusion matrix table
prob=predict(fit,type = c("response"),claimants)
prob
prob
output<-cbind(claimants,prob)
View(output)
confusion<-table(output$ATTORNEY,output$prob>0.50)
confusion
accuracy<-sum(diag(confusion))/sum(confusion)
accuracy
library(randomForest)
table(is.na(claimants$CLMAGE))
claimants<-na.roughfix(claimants)
fit<-glm(ATTORNEY~CLMAGE+CLMINSUR+CLMSEX+SEATBELT+LOSS)
summary(fit)
exp(coef(fit))
#conclusion matrix table
prob=predict(fit,type = c("response"),claimants)
prob
output<-cbind(claimants,prob)
View(output)
confusion<-table(output$ATTORNEY,output$prob>0.50)
confusion
accuracy<-sum(diag(confusion))/sum(confusion)
accuracy
output<-cbind(claimants,prob)
View(output)
confusion<-table(output$ATTORNEY,output$prob>0.50)
confusion
accuracy<-sum(diag(confusion))/sum(confusion)
accuracy
library(readr)
claimants <- read_csv("D:/R Datasets/claimants.csv")
View(claimants)
claimants<-claimants[-1]
View(claimants)
table(is.na(claimants$CLMAGE))
libabry(randomForest)
libabry(randomForest())
library(randomForest())
a<-na.roughfix(claimants)
table(is.na(a$CLMAGE))
attach(a)
temp<-glm(ATTORNEY~CLMAGE+CLMINSUR+CLMSEX+SEATBELT+LOSS)
summary(temp)
exp(coef(temp))
prb<-predict(temp,type = c("response"),a)
prb
output<-cbind(a,prb)
View(output)
confusion<-table(output$ATTORNEY,output$prb>0.5)
confusion
accuracy<-sum(diag(confusion))/sum(confusion)
accuracy
library(readr)
claimants <- read_csv("D:/R Datasets/claimants.csv")
View(claimants)
claimants<-claimants[-1]
#program for logistic regression
attach(claimants)
fit<-glm(ATTORNEY~CLMSEX+CLMINSUR+SEATBELT+CLMAGE+LOSS,family = binomial,data = claimants)
summary(fit)
#the value we get from summary are log values, so we need to change to actual values
exp(coef(fit))
#confusion matrix table
prob=predict(fit,type=c("response"),claimants)
prob
output<-cbind(claimants,prob)
output
confusion<-table(output$ATTORNEY,output$prob>0.50)
confusion
accuracy<-sum(diag(confusion))/sum(confusion)
accuracy
library(readr)
claimants <- read_csv("D:/R Datasets/claimants.csv")
View(claimants)
claimants<-claimants[-1]
View(claimants)
table(is.na(claimants$CLMAGE))
library(randomForest())
a<-na.roughfix(claimants)
table(is.na(a$CLMAGE))
attach(a)
temp<-glm(ATTORNEY~CLMAGE+CLMINSUR+CLMSEX+SEATBELT+LOSS,family = binomial(),a)
summary(temp)
exp(coef(temp))
prb<-predict(temp,type = c("response"),a)
prb
output<-cbind(a,prb)
View(output)
confusion<-table(output$ATTORNEY,output$prb>0.5)
confusion
accuracy<-sum(diag(confusion))/sum(confusion)
accuracy
plot(confusion)
plot(a)
plot(temp)
plot(temp)
temp<-glm(ATTORNEY~CLMAGE+CLMINSUR+CLMSEX+SEATBELT+LOSS,family = binomial(),a)
summary(temp)
temp<-glm(ATTORNEY~CLMAGE+CLMINSUR+CLMSEX+LOSS,family = binomial(),a)
summary(temp)
exp(coef(temp))
prb<-predict(temp,type = c("response"),a)
prb
output<-cbind(a,prb)
View(output)
confusion<-table(output$ATTORNEY,output$prb>0.5)
confusion
accuracy<-sum(diag(confusion))/sum(confusion)
accuracy
plot(temp)
View(iris)
attach(iris)
dummy_species<-data.frame(model.matrix(~Species-1,data = iris))
View(dummy_species)
iris<-cbind(iris,dummy_species)
View(iris)
iris<-iris[-5]
View(iris)
View(iris)
attach(iris)
rudra<-data.frame(model.matrix(~Species,data=iris))
View(rudra)
iris<-cbind(iris,rudra)
View(iris)
iris<-iris[-5]
View(iris)
cor(iris)
View(iris)
attach(iris)
chand<-as.data.frame(model.matrix(~Species-1,data = iris))
chand
iris<-cbind(iris,chand)
View(iris)
iris[-5]
View(iris)
iris=iris[-5]
View(iris)
?iris
View(iris)
attach(iris)
rudra<-as.data.frame(model.matrix(Species-1,data = iris))
rudra<-as.data.frame(model.matrix(Species-1,data = iris))
rudra<-as.data.frame(model.matrix(Species,data = iris))
attach(iris)
chand<-as.data.frame(model.matrix(~Species-1.data=iris))
chand<-as.data.frame(model.matrix(~Species-1,data=iris))
iris<-cbind(iris,chand)
View(iris)
iris<-iris[-5]
View(iris)
library(readr)
claimants <- read_csv("D:/R Datasets/claimants.csv")
View(claimants)
claimants<-claimants[-1]
View(claimants)
dr<-glm(ATTORNEY~CLMAGE+CLMINSUR+CLMSEX+SEATBELT+LOSS)
summary(dr)
exp(coef(dr))
dr<-glm(ATTORNEY~CLMAGE+CLMINSUR+CLMSEX+SEATBELT+LOSS,family = binomial(),claimants)
summary(dr)
exp(coef(dr))
pr<-predict(dr,type=c("response"),claimants)
pr
output<-cbind(claimants,pr)
View(output)
confusion<-table(output$ATTORNEY,output$pr>0.50)
confusion
accuracy<-sum(diag(confusion))/sum(confusion)
accuracy
plot(accuracy)
plot(dr)
Cars <- read.csv("")
View(Cars)
library(readr)
Cars <- read_csv("D:/R Datasets/Multi linear regression/Multi linear regression/Cars.csv")
View(Cars)
Cars <- read.csv("")
View(Cars)
attach(Cars)
summary(Cars)
install.packages("GGally")
install.packages("GGally")
# 7. Find the correlation b/n Output (MPG) & (HP,VOL,SP)-Scatter plot
pairs(Cars)
library(GGally)
ggpairs(Cars)
library(ggplot2)
ggpairs(Cars)
# 8. Correlation Coefficient matrix - Strength & Direction of Correlation
cor(Cars)
### Partial Correlation matrix - Pure Correlation  b/n the varibles
install.packages("corpcor")
library(corpcor)
cor2pcor(cor(Cars))
# The Linear Model of interest
model.car <- lm(MPG~VOL+HP+SP+WT)
summary(model.car)
# Prediction based on only Volume
model.carV<-lm(MPG~VOL)
summary(model.carV) # Volume became significant
# Prediction based on only Weight
model.carW<-lm(MPG~WT)
summary(model.carW) # Weight became significant
# Prediction based on Volume and Weight
model.carVW<-lm(MPG~VOL+WT)
summary(model.carVW) # Both became Insignificant
# So there exists a collinearity problem b/n volume and weight
### Scatter plot matrix along with Correlation Coefficients
panel.cor<-function(x,y,digits=2,prefix="",cex.cor)
{
usr<- par("usr"); on.exit(par(usr))
par(usr=c(0,1,0,1))
r=(cor(x,y))
txt<- format(c(r,0.123456789),digits=digits)[1]
txt<- paste(prefix,txt,sep="")
if(missing(cex.cor)) cex<-0.4/strwidth(txt)
text(0.5,0.5,txt,cex=cex)
}
pairs(Cars,upper.panel = panel.cor,main="Scatter plot matrix with Correlation coefficients")
# It is Better to delete influential observations rather than deleting entire column which is
# costliest process
# Deletion Diagnostics for identifying influential observations
influence.measures(model.car)
library(car)
## plotting Influential measures
influenceIndexPlot(model.car,id.n=3) # index plots for infuence measures
influencePlot(model.car,id.n=3) # A user friendly representation of the above
# Regression after deleting the 77th observation, which is influential observation
model.car1<-lm(MPG~VOL+SP+HP+WT,data=Cars[-77,])
summary(model.car1)
View(model.car1)
summary(model.car1)
# Regression after deleting the 77th & 71st Observations
model.car2<-lm(MPG~VOL+SP+HP+WT,data=Cars[-c(71,77),])
summary(model.car2)
## Variance Inflation factor to check collinearity b/n variables
vif(model.car)
## Added Variable plot to check correlation b/n variables and o/p variable
avPlots(model.car,id.n=2,id.cex=0.7)
## Final model
finalmodel<-lm(MPG~VOL+SP+HP,data = Cars[-77,])
summary(finalmodel)
# Evaluate model LINE assumptions
plot(finalmodel)
#Residual plots,QQplot,std-Residuals Vs Fitted,Cook's Distance
qqPlot(model.car,id.n = 5)
library(readr)
Cars <- read_csv("D:/R Datasets/Multi linear regression/Multi linear regression/Cars.csv")
View(Cars)
View(Cars)
attach(Cars)
cor(Cars)
rudra<-lm(MPG~HP+VOL+SP+WT)
summary(rudra)
rudrav<-lm(MPG~VOL)
Summary(rudrav)
summary(rudrav)
rudraw<-lm(MPG~WT)
summary(rudraw)
rudravw<-lm(MPG~VOL+WT)
summary(rudravw)
influence.measures(rudra)
??influenceIndexPlot()
library(CARS)
library(CAR)
influenceIndexPlot(rudra,id.n = 3)
rudra1<-lm(MPG~HP+VOL+SP+WT,data=Cars[-77])
rudra1<-lm(MPG~HP+VOL+SP+WT,data=Cars[-77,])
summary(rudra1)
rudra1<-lm(MPG~HP+VOL+SP+WT,data=Cars[-77,71])
rudra1<-lm(MPG~HP+VOL+SP+WT,data=Cars[-(77,71),])
rudra1<-lm(MPG~HP+VOL+SP+WT,data=Cars[-c(77,71),])
summary(rudra1)
vif(rudra)
## Added Variable plot to check correlation b/n variables and o/p variable
avPlots(rudra,id.n=2,id.cex=0.7)
final<-lm(MPG~HP+VOL+SP)
summary(final)
plot(final)
qqplot(final)
qqplot(final,id.n=5)
qqPlot(final,id.n = 5)
qqPlot(final,id.n = 3)
library(readr)
wbcd <- read_csv("D:/R Datasets/Machine Learning/Supervised Learning/KNN/wbcd.csv")
View(wbcd)
(wbcd)
View(wbcd)
str(wbcd)
dim(wbcd)
# Id column can be removed
wbcd<-wbcd[-1]
dim(wbcd)
# count of types of cancer
table(wbcd$diagnosis)
# Converting the diagnosis into factor and assigning M as Malignant and B as Benign
wbcd$diagnosis<-factor(wbcd$diagnosis,levels = c("B","M"),labels = c("Benign","Malignant"))
# count of B and M
table(wbcd$diagnosis)
# Proportions of B and M
round(prop.table(table(wbcd$diagnosis))*100,digits=1)
# rest of the data contains 3 different measures of 10 characteristics
summary(wbcd[c("radius_mean","area_mean","smoothness_mean")])
# From the above data it seems that area will show more influence than smoothness. So we will normalize the data
normalize_data<-function(x){
return((x-min(x))/(max(x)-min(x)))
}
# Rather than applying each element individually we will use "lapply()" function in R which
# it considers the list and applies the function to each list element
# Lapply function considers each column as list and it will apply function to each element under columns
wbcd_n<-as.data.frame(lapply(wbcd[2:31],normalize_data))
summary(wbcd_n$area_mean)
# for predicting the accuracy of the model we will first split the data into training and testing
# training - 469 and test data contains -100 data
# [ row column ] <- syntax
wbcd_train<-wbcd_n[1:469,]
dim(wbcd_train)
wbcd_test<-wbcd_n[470:569,]
dim(wbcd_test)
wbcd_train_labels<-wbcd[1:469,1]
wbcd_train_labels<-as.factor(wbcd_train_labels[[1]])
class(wbcd_train_labels)
wbcd_test_labels<-wbcd[470:569,1]
wbcd_test_labels<-as.factor(wbcd_test_labels[[1]])
class(wbcd_test_labels)
# class package for classification which is a k-nn implementation and we will use knn() to to implement
# knn algorithm. knn() takes 4 parameters - train,test,class,k
library(class)
wbcd_test_pred<-knn(wbcd_train,wbcd_test,cl=wbcd_train_labels,k=21) # sqrt(469) generalized k-value odd number
class(wbcd_test_pred)
# evaluating the model performance
# wbcd_test_pred vector should match with wbcd_test_labels
# By crosstable function from gmodels package
library(gmodels)
# cross table
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,
prop.chisq=FALSE)
wbcd_test_pred<-knn(wbcd_train,wbcd_test,cl=wbcd_train_labels,k=27) # sqrt(469) generalized k-value odd number
class(wbcd_test_pred)
# evaluating the model performance
# wbcd_test_pred vector should match with wbcd_test_labels
# By crosstable function from gmodels package
library(gmodels)
# cross table
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,
prop.chisq=FALSE)
library(readr)
wbcd <- read_csv("D:/R Datasets/Machine Learning/Supervised Learning/KNN/wbcd.csv")
View(wbcd)
wbcd<-wbcd[-1]
attach(wbcd)
table(diagnosis)
class(diagnosis)
diagnosis<-factor(diagnosis,levels = c("B","M"),labels = c("Benign","Malignant"))
View(wbcd)
class(diagnosis)
View(wbcd)
diagnosis<-factor(diagnosis,levels = c("B","M"),labels = c("Benign","Mlignant"))
View(wbcd)
diagnosis<-(diagnosis,levels = c("B","M"),labels = c("Benign","Mlignant"))
diagnosis<-(wbcd$diagnosis,levels = c("B","M"),labels = c("Benign","Mlignant"))
class(diagnosis)
table(diagnosis)
library(readr)
wbcd <- read_csv("D:/R Datasets/Machine Learning/Supervised Learning/KNN/wbcd.csv")
View(wbcd)
